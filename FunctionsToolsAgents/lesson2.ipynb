{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c349b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f7774d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e385b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf868c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://models.github.ai/inference\"\n",
    "model_name = \"openai/gpt-4o\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=os.environ['GITHUB_API_TOKEN'],\n",
    "    )\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of England?\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5794fe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of England is **London**.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161154f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e65081",
   "metadata": {},
   "source": [
    "### Simple chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f7abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}.\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(api_key=openai_api_key,temperature=0.0, model=model)\n",
    "outputParser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bac8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | outputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950acca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! Here's a cat joke for you:\\n\\nWhy was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse! üê±üñ±Ô∏è\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aabc7a",
   "metadata": {},
   "source": [
    "### More complex Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11083dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "debd9fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.landoulsi\\Desktop\\pprojects\\langchain\\.venv\\lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"Fares worked at Orenda\", \"chimps are mammals\"],\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\",dimensions=1024)\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eca2bc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m.landoulsi\\AppData\\Local\\Temp\\ipykernel_26044\\48198759.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(\"Where did Fares work?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Fares worked at Orenda'),\n",
       " Document(metadata={}, page_content='chimps are mammals')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Where did Fares work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc46f37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='chimps are mammals'),\n",
       " Document(metadata={}, page_content='Fares worked at Orenda')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Who are chimps?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41e81dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" Answer the question based on the context provided. \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question} \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01275640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0bd6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\" : lambda x : retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\" : lambda x : x[\"question\"]\n",
    "}) | prompt | model | outputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a609720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fares worked at Orenda.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Where did Fares work?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7cbf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = RunnableMap({\n",
    "    \"context\" : lambda x : retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\" : lambda x : x[\"question\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc70a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='Fares worked at Orenda'),\n",
       "  Document(metadata={}, page_content='chimps are mammals')],\n",
       " 'question': 'Where did Fares work?'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.invoke({\"question\": \"Where did Fares work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c1f66",
   "metadata": {},
   "source": [
    "### Bind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c12cccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca60acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(api_key=openai_api_key,temperature=0.0, model=model).bind(functions=functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "968fe339",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0e999f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'weather_search'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 64, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BZZTNf9LbOYc6K6eeVrS0uuUVV3V3', 'service_tier': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run--9748853f-9912-44f5-9136-a0a8b95fa963-0', usage_metadata={'input_tokens': 64, 'output_tokens': 17, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"What is the weather at SFO?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecd267b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60452a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dd32181",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6180bca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"team_name\":\"Esperance de Tunis\"}', 'name': 'sports_search'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 99, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BZZV1bdhoCDmIPSPtS3uGlCVERvjf', 'service_tier': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run--5aa318d4-1476-47a8-b7bc-77c48ac8d4f5-0', usage_metadata={'input_tokens': 99, 'output_tokens': 20, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"How did Esperance de Tunis do in their last game?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1501469",
   "metadata": {},
   "source": [
    "### Fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "simple_model = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.0,\n",
    "        openai_api_key=openai_api_key,\n",
    ")\n",
    "simple_chain = simple_model | (lambda x: json.loads(x.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cebe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0180362b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a JSON blob containing three poems, each represented as a JSON object with a title, author, and first line:\\n\\n```json\\n{\\n  \"poems\": [\\n    {\\n      \"title\": \"Whispers of the Dawn\",\\n      \"author\": \"Emily Carter\",\\n      \"first_line\": \"In the hush of morning light, dreams take flight.\"\\n    },\\n    {\\n      \"title\": \"The Dance of Leaves\",\\n      \"author\": \"Michael Thompson\",\\n      \"first_line\": \"As autumn winds begin to weave, the trees start to sway.\"\\n    },\\n    {\\n      \"title\": \"Echoes of the Sea\",\\n      \"author\": \"Sophia Lee\",\\n      \"first_line\": \"Beneath the waves, a story sleeps, in whispers soft and deep.\"\\n    }\\n  ]\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 31, 'total_tokens': 198, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_7a53abb7a2', 'id': 'chatcmpl-BZZdJxchqUUWRxfBZZuOFcz02Ggg9', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--043a153c-9b16-490f-b79a-2bbec68edcfb-0', usage_metadata={'input_tokens': 31, 'output_tokens': 167, 'total_tokens': 198, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cd41293",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msimple_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchallenge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.landoulsi\\Desktop\\pprojects\\langchain\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3034\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3033\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3034\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\m.landoulsi\\Desktop\\pprojects\\langchain\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4757\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4743\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[0;32m   4744\u001b[0m \n\u001b[0;32m   4745\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4754\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[0;32m   4759\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4760\u001b[0m         ensure_config(config),\n\u001b[0;32m   4761\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4762\u001b[0m     )\n\u001b[0;32m   4763\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4764\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "File \u001b[1;32mc:\\Users\\m.landoulsi\\Desktop\\pprojects\\langchain\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1930\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   1928\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1929\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m-> 1930\u001b[0m             context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1931\u001b[0m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1932\u001b[0m                 func,\n\u001b[0;32m   1933\u001b[0m                 \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1934\u001b[0m                 config,\n\u001b[0;32m   1935\u001b[0m                 run_manager,\n\u001b[0;32m   1936\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1937\u001b[0m             ),\n\u001b[0;32m   1938\u001b[0m         )\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1940\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\m.landoulsi\\Desktop\\pprojects\\langchain\\.venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    427\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\m.landoulsi\\Desktop\\pprojects\\langchain\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4615\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4613\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   4614\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4615\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m   4616\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   4617\u001b[0m     )\n\u001b[0;32m   4618\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[0;32m   4619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32mc:\\Users\\m.landoulsi\\Desktop\\pprojects\\langchain\\.venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    427\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m simple_chain \u001b[38;5;241m=\u001b[39m simple_model \u001b[38;5;241m|\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m simple_chain\u001b[38;5;241m.\u001b[39minvoke(challenge)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "simple_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f5ce22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"o3-mini\"\n",
    "model = ChatOpenAI(\n",
    "        model=model,\n",
    "        openai_api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "chain = model | (lambda x: json.loads(x.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a79d02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"poems\": [\\n    {\\n      \"title\": \"The Whispering Woods\",\\n      \"author\": \"John Doe\",\\n      \"first_line\": \"Beneath the ancient canopy, secrets whisper softly.\"\\n    },\\n    {\\n      \"title\": \"Sunrise Epiphany\",\\n      \"author\": \"Jane Smith\",\\n      \"first_line\": \"As dawn breaks, hope floods the horizon in golden hues.\"\\n    },\\n    {\\n      \"title\": \"Nocturnal Lullaby\",\\n      \"author\": \"Alex Rivers\",\\n      \"first_line\": \"In the quiet dark, the stars murmur forgotten melodies.\"\\n    }\\n  ]\\n}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 847, 'prompt_tokens': 30, 'total_tokens': 877, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_ded0d14823', 'id': 'chatcmpl-BZZq272ePgz50pM8gLcqzul8HYVog', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--d8aceeb9-9b21-4120-9bf4-4b676c745eae-0', usage_metadata={'input_tokens': 30, 'output_tokens': 847, 'total_tokens': 877, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7917a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poems': [{'title': \"The Dawn's Whisper\",\n",
       "   'author': 'Melody Rivers',\n",
       "   'first_line': 'In the quiet blush of dawn, secrets softly unfurl.'},\n",
       "  {'title': 'Midnight Echo',\n",
       "   'author': 'Nightshade Monroe',\n",
       "   'first_line': 'The moon murmurs its tales in the silent embrace of night.'},\n",
       "  {'title': \"Autumn's Lament\",\n",
       "   'author': 'Fiona Swift',\n",
       "   'first_line': 'Falling leaves echo the sorrows of a bygone summer.'}]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52cde170",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = simple_chain.with_fallbacks([chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b02c9102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poems': [{'title': 'The Silent River',\n",
       "   'author': 'Aiden Brooks',\n",
       "   'first_line': 'In the quiet of midnight, the river whispers ancient secrets.'},\n",
       "  {'title': \"Dawn's Tender Light\",\n",
       "   'author': 'Isabella Reed',\n",
       "   'first_line': 'At first blush, the sky unfurls a soft embrace of golden hope.'},\n",
       "  {'title': 'Echoes of the Past',\n",
       "   'author': 'Evelyn Cruz',\n",
       "   'first_line': 'Beneath crumbling arches, time murmurs stories of lost dreams.'}]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291e879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
