{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "# print(token)\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of England?\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 26 \n",
    "chunk_overlap = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\n",
    "c_splitter = CharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                    chunk_overlap=chunk_overlap)\n",
    "r_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                            chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m n o p q r s t u v w x y z']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                chunk_overlap=chunk_overlap,\n",
    "                                separator=\" \")\n",
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive splitting details\n",
    "\n",
    "`RecursiveCharacterTextSplitter` is recommended for generic text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(chunk_size=450,\n",
    "                                chunk_overlap=0,\n",
    "                                separator=\" \")\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(chunk_size=450,\n",
    "                                            chunk_overlap=0,\n",
    "                                            separators=[\"\\n\\n\",\"\\n\", \" \",\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related\",\n",
       " '. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns',\n",
       " '. Carriage returns are the \"backslash n\" you see embedded in this string',\n",
       " '. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"motivation.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\n \\n\" \"(?<=\\. )\", \" \", \"\"],\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'motivation.pdf', 'page': 0}, page_content='I  am  Mohamed  Fares  Landoulsi,  AI  engineer  within  Novobit,  a  software  company  located  in  \\nBraunschweig,\\n \\nGermany.\\n \\nI\\n \\ngraduated\\n \\nin\\n \\nDecember\\n \\n2023\\n \\nas\\n \\na\\n \\nsoftware\\n \\nengineer\\n \\nfrom\\n \\nthe\\n \\nHigher\\n \\nInstitute\\n \\nof\\n \\nComputer\\n \\nScience\\n \\nAriana,\\n \\nin\\n \\nTunisia,\\n \\nafter\\n \\na\\n \\nsuccessful\\n \\nend\\n \\nof\\n \\nstudies\\n \\ninternship\\n \\nin\\n \\nNovobit\\n \\nin\\n \\nGermany\\n \\nthat\\n \\nled\\n \\nto\\n \\nbeing\\n \\nhired.\\n  My  end  of  studies  internship  lasted  about  6  months  from  May  2023  to  November  2023,  in  which  \\nI\\n \\napplied\\n \\nmy\\n \\nknowledge\\n \\nin\\n \\ndeep\\n \\nlearning\\n \\nto\\n \\nbuild\\n \\nsmart\\n \\nglasses\\n \\nfor\\n \\nvisually\\n \\nimpaired\\n \\npersons,\\n \\nin\\n \\na\\n \\nteam\\n \\nof\\n \\n8\\n \\npersons.\\n \\nMy\\n \\nresponsibility\\n \\nwas\\n \\nto\\n \\nbuild\\n \\nan\\n \\nobstacle\\n \\ndetection\\n \\nand\\n \\navoidance\\n \\nsystem\\n \\nthat\\n \\nwould\\n \\nbe\\n \\nintegrated\\n \\nwith\\n \\nthe\\n \\nother\\n \\nfunctionalities\\n \\nof\\n \\nthe\\n \\nsmart\\n \\nglasses,\\n \\nsuch\\n \\nas\\n \\nhand\\n \\ngestures\\n \\nrecognition.\\n \\nWithin\\n \\nthis\\n \\ntime\\n \\nframe,\\n \\nI\\n \\ngot\\n \\nfamiliar\\n \\nwith\\n \\ntransformer'),\n",
       " Document(metadata={'source': 'motivation.pdf', 'page': 0}, page_content='of\\n \\nthe\\n \\nsmart\\n \\nglasses,\\n \\nsuch\\n \\nas\\n \\nhand\\n \\ngestures\\n \\nrecognition.\\n \\nWithin\\n \\nthis\\n \\ntime\\n \\nframe,\\n \\nI\\n \\ngot\\n \\nfamiliar\\n \\nwith\\n \\ntransformer\\n \\nbased\\n \\nmodels\\n \\nand\\n \\narchitectures\\n \\nutilizing\\n \\ngrounding\\n \\nDino\\n \\nfor\\n \\nzero\\n \\nshot\\n \\nobject\\n \\ndetection,\\n \\nBLIP\\n \\nfor\\n \\nimage\\n \\ncaptioning,\\n \\nYolov8\\n \\nfor\\n \\nobject\\n \\ndetection.\\n  I  was  also  in  charge  of  the  interactive  side  of  the  smart  glasses  by  providing  a  voice  assistant  \\nand\\n \\nWake\\n \\nword\\n \\ntechnology\\n \\nto\\n \\nreceive\\n \\nthe\\n \\nvoice\\n \\nrequests\\n \\nof\\n \\nthe\\n \\nimpaired\\n \\nperson,\\n \\napplying\\n \\ntechniques\\n \\nsuch\\n \\nas\\n \\nintent\\n \\nclassification\\n \\nusing\\n \\nsimple\\n \\nRNNs\\n \\nand\\n \\nmel\\n \\nspectrograms\\n \\nclassification\\n \\nusing\\n \\na\\n \\ndeep\\n \\nneural\\n \\nnetwork\\n \\nto\\n \\ndetect\\n \\nthe\\n \\nwake\\n \\nword.\\n  Additionally,  I  have  done  benchmarks  on  the  available  tools  that  could  support  our  system  in  an  \\nembedded\\n \\ntool,\\n \\nending\\n \\nwith\\n \\nNvidia\\n \\nJetson\\n \\nOrin\\n \\nembedded\\n \\nboard\\n \\nand\\n \\nNvidia\\n \\nIntel\\n \\nRealSense\\n \\nas\\n \\na\\n \\ncamera'),\n",
       " Document(metadata={'source': 'motivation.pdf', 'page': 0}, page_content='embedded\\n \\ntool,\\n \\nending\\n \\nwith\\n \\nNvidia\\n \\nJetson\\n \\nOrin\\n \\nembedded\\n \\nboard\\n \\nand\\n \\nNvidia\\n \\nIntel\\n \\nRealSense\\n \\nas\\n \\na\\n \\ncamera\\n \\nthat\\n \\nprovides\\n \\nnot\\n \\njust\\n \\nRGB\\n \\nimages\\n \\nbut\\n \\nalso\\n \\ndepth\\n \\nestimation\\n \\nso\\n \\nthat\\n \\nwe\\n \\ncan\\n \\ndetect\\n \\nhow\\n \\nclose\\n \\nthe\\n \\nobjects\\n \\nare\\n \\nthe\\n \\nglasses\\n \\nand\\n \\nwarn\\n \\nthe\\n \\nvisually\\n \\nimpaired\\n \\nperson\\n \\nwhen\\n \\nthere\\n \\nis\\n \\na\\n \\npotential\\n \\nobstacle.\\n  Moving  forward  to  2024,  I  am  currently  working  with  the  same  company  as  an  AI  engineer  on  \\nanother\\n \\nproject\\n \\nthat\\n \\nwill\\n \\nempower\\n \\ndevelopers\\n \\nby\\n \\nhelping\\n \\nthem\\n \\ncreate\\n \\nsoftware\\n \\napplications\\n \\nusing\\n \\nour\\n \\nAI\\n \\nplatform\\n \\nthat\\n \\ngenerates\\n \\nfrontend\\n \\nand\\n \\nbackend\\n \\napplications\\n \\nbased\\n \\non\\n \\nLarge\\n \\nLanguage\\n \\nModels.\\n  Throughout  this  journey,  I  have  used  both  open  source  and  closed  source  LLMs  and  took  \\ndifferent\\n \\nresponsibilities,\\n \\nusing\\n \\ndifferent\\n \\ntools,\\n \\nmodels\\n \\nand\\n \\ntechnologies\\n \\nsuch\\n \\nas\\n \\nFlask,\\n \\nFastAPI,\\n \\nLlama3.3,\\n \\nClaude,'),\n",
       " Document(metadata={'source': 'motivation.pdf', 'page': 0}, page_content='different\\n \\nresponsibilities,\\n \\nusing\\n \\ndifferent\\n \\ntools,\\n \\nmodels\\n \\nand\\n \\ntechnologies\\n \\nsuch\\n \\nas\\n \\nFlask,\\n \\nFastAPI,\\n \\nLlama3.3,\\n \\nClaude,\\n \\nGPT-4o,\\n \\nRoBERTa\\n \\nand\\n \\ngained\\n \\nmany\\n \\ninsights\\n \\nin\\n \\nprompt\\n \\nengineering,\\n \\nRetrieval\\n \\naugmented\\n \\ngeneration,\\n \\nLLM\\n \\ndeployments\\n \\nusing\\n \\nOllama\\n \\nand\\n \\nLlama.cpp.\\n \\nI\\n \\ncan\\n \\nsay\\n \\nit\\n \\nwas\\n \\na\\n \\nlong\\n \\njourney\\n \\nof\\n \\nexploration\\n \\nand\\n \\ntrials\\n \\nand\\n \\nerrors\\n \\nthat\\n \\ntaught\\n \\nme\\n \\na\\n \\nlot\\n \\nin\\n \\na\\n \\nvery\\n \\nshort\\n \\ntime.\\n Back  to  the  years  before  2023,  I  had  many  different  internships  ranging  from  internships  in  web  \\ndevelopment,\\n \\nblockchain\\n \\ntechnologies,\\n \\nNLP\\n \\nand\\n \\nworked\\n \\non\\n \\nmany\\n \\nacademic\\n \\nand\\n \\nside\\n \\nprojects\\n \\nthroughout\\n \\nmy\\n \\nsix\\n \\nyears\\n \\n(from\\n \\n2018\\n \\nto\\n \\n2023)\\n \\nat\\n \\nthe\\n \\nuniversity\\n \\nin\\n \\nTunisia.\\n  Finally,  I  find  a  huge  interest  in  diving  deep  into  machine  learning  and  large  language  models  \\nand\\n \\ngenerative\\n \\nAI\\n \\nin\\n \\ngeneral\\n \\nand\\n \\nI\\n \\nfind\\n \\nthat\\n \\nbeing\\n \\nin\\n \\nyour'),\n",
       " Document(metadata={'source': 'motivation.pdf', 'page': 0}, page_content='and\\n \\ngenerative\\n \\nAI\\n \\nin\\n \\ngeneral\\n \\nand\\n \\nI\\n \\nfind\\n \\nthat\\n \\nbeing\\n \\nin\\n \\nyour\\n \\ncompany\\n \\nwill\\n \\ngive\\n \\nme\\n \\nbetter\\n \\nand\\n \\nin\\n \\ndepth\\n \\ninsights\\n \\nabout\\n \\nthese\\n \\nlatest\\n \\nadvancements\\n \\nin\\n \\nAI\\n \\nand\\n \\nprovide\\n \\nme\\n \\nwith\\n \\na\\n \\nhuge\\n \\nopportunity\\n \\nto\\n \\nbuild\\n \\nmore\\n \\ntools\\n \\nfor\\n \\nthe\\n \\npeople\\n \\nand\\n \\nthe\\n \\ncommunity.')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "\n",
    "loader = NotionDirectoryLoader(\"notion\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'notion\\\\Self study & Improvement.md'}, page_content='# Self study & Improvement\\n\\nI want to outline my study plans to grow in artificial intelligence as an AI engineer. \\n\\nThis plan is more focused on long term growth with solid understanding of the basics and competence in translating foundational knowledge into clean and professional code. \\n\\n### **Goals of the self study plans :**\\n\\n- Become proficient at Pytorch while implementing the state of art breakthrough in AI.\\n- Get deep insights and foundational knowledge about NLPs, Transformers, finetuning, LLMs\\n- Get so familiar with vector databases and RAG ( Retrieval augmented generation)\\n\\n### **Strategy :**\\n\\nTo be able to achieve the aforementioned goals, a strategy will come with measurable actions and deeds that will be exercised on daily/weekly basis. \\n\\nThis strategy consists of varying the resources of learning, from courses, tutorials, articles to youtube videos, and personal projects.'),\n",
       " Document(metadata={'source': 'notion\\\\Self study & Improvement.md'}, page_content='This strategy consists of varying the resources of learning, from courses, tutorials, articles to youtube videos, and personal projects. \\n\\nFor the 3 upcoming months, I will try to invest 50% of my time in PyTorch Tutorials since I need to get a skill brushing in deep learning and Tensors and code Optimization. \\n\\nThe other 50% of the time will be devided between youtube video courses ( such as the great tutorials of Andrej Karpathy) and deepLearningAI courses in RAG, vector databases which will consolidate my high level knowledge in NLP and LLMs and self tailored projects that would put my learning to action. \\n\\n**Special Cases :** \\n\\nPreparing for a technical interview might need that I change the strategy a bit depending on the requirements of that interview, of example I speed up in information gathering to be ready to discuss more broader topics and respond to more varied questions'),\n",
       " Document(metadata={'source': 'notion\\\\Self study & Improvement.md'}, page_content='If everything works well in these 3 months, I might incorporate more in depth learning from deep learning books that covers more mathematics and in depth foundational background. \\n\\n**Remarks :** \\n\\nPytorch Tutorials : not always obvious what to tutorial to take, it needs most of the time research to know what’s the next step. I might use an AI assistant to help me figure out what I exactly need in each moment.\\n\\nDefining an estimated weekly target is important to be able to track how far I could have progressed. Also a weekly report to see what was achieved and what not and what might have been the issues to be adjusted in the next weeks.\\n\\n**Current schedule and interview/coding test prep :**\\n\\nOn a daily basis, I think working on my personal project in the morning for about **an 1 hour.** Dive in Pytorch for **1 hour.** If possible I would try to get a course tutorial for **1 hour** done as well in the morning/ evening. Otherwise, we keep it at night after work. \\n\\n@mariem kallel'),\n",
       " Document(metadata={'source': 'notion\\\\Self study & Improvement.md'}, page_content='@mariem kallel \\n\\nWeeks planning and reports\\n\\n[Weeks](https://www.notion.so/Weeks-1a0240e411ba81bfb7a3fdf2888c4428?pvs=21)\\n\\nDaily tasks will be outlined here\\n\\n[Daily interests and deeds :](https://www.notion.so/Daily-interests-and-deeds-1a0240e411ba812d8e1cd3270d522e44?pvs=21)\\n\\n### **Milestones :**\\n\\n-Accumulate a good knowledge in deep learning, a decent number of courses and a certain know how in approaching Deep learning problems.\\n\\n-Be able to confidently write PyTorch code for simple deep learning use cases\\n\\n-Be able to read simple research papers’ implementation written in PyTorch\\n\\n-Be able to write a production level code for deep learning use cases.\\n\\n[Resources ](https://www.notion.so/Resources-1a0240e411ba81ec991dcc117d59bc1e?pvs=21)')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
