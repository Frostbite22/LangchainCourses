{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "# print(token)\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of England?\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"docs/chroma2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m.landoulsi\\AppData\\Local\\Temp\\ipykernel_14248\\1143856407.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(metadata={}, page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(metadata={}, page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Diversity: Maximum marginal relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those homeworks will be done in either MATLAB or in Octave, which is sort of — I \\nknow some people c'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into his office and he said, \"Oh, professor, professor, thank you so much for your \\nmachine learning'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those homeworks will be done in either MATLAB or in Octave, which is sort of — I \\nknow some people c'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reasonable choice on many problems, but you can actually plug in other functions as \\nwell. Did I men'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Specificity: working with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=5,\n",
    "    filter={\"source\":\"MachineLearning-Lecture03.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'MachineLearning-Lecture03.pdf'}\n",
      "{'page': 14, 'source': 'MachineLearning-Lecture03.pdf'}\n",
      "{'page': 6, 'source': 'MachineLearning-Lecture03.pdf'}\n",
      "{'page': 2, 'source': 'MachineLearning-Lecture03.pdf'}\n",
      "{'page': 13, 'source': 'MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Specificity: working with metadata using self-query retriever\n",
    "\n",
    "But we have an interesting challenge: we often want to infer the metadata from the query itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    " \n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in as well\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `MachineLearning-Lecture01.pdf`, `MachineLearning-Lecture02.pdf`, or `MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Lecture notes\"\n",
    "llm = OpenAI(model=\"gpt-4o\",temperature=0.0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 2, 'source': 'MachineLearning-Lecture03.pdf'}\n",
      "{'page': 10, 'source': 'MachineLearning-Lecture03.pdf'}\n",
      "{'page': 11, 'source': 'MachineLearning-Lecture03.pdf'}\n",
      "{'page': 6, 'source': 'MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression\n",
    "\n",
    "Another approach for improving the quality of retrieved docs is compression.\n",
    "\n",
    "Information most relevant to a query may be buried in a document with a lot of irrelevant text. \n",
    "\n",
    "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o\",temperature=0.0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      ">>>\n",
      "those homeworks will be done in either MATLAB or in Octave, which is sort of — I \n",
      "know some people call it a free version of MATLAB, which it sort of is, sort of isn't.  \n",
      "So I guess for those of you that haven't seen MATLAB before, and I know most of you \n",
      "have, MATLAB is I guess part of the programming language that makes it very easy to \n",
      "write codes using matrices, to write code for numerical routines, to move data around, to \n",
      "plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of \n",
      "learning algorithms.  \n",
      "And in case some of you want to work on your own home computer or something if you \n",
      "don't have a MATLAB license, for the purposes of this class, there's also — [inaudible] \n",
      "write that down [inaudible] MATLAB — there' s also a software package called Octave \n",
      "that you can download for free off the Internet. And it has somewhat fewer features than \n",
      "MATLAB, but it's free, and for the purposes of this class, it will work for just about \n",
      "everything.  \n",
      ">>>\n",
      "\n",
      "> Question: what did they say about octave?\n",
      "> Context:\n",
      ">>>\n",
      "those homeworks will be done\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      ">>>\n",
      "So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, \n",
      "and we'll actually have a short MATLAB tutorial in one of the discussion sections for \n",
      "those of you that don't know it.  \n",
      ">>>\n",
      "\n",
      "> Question: what did they say about the discussion sections?\n",
      "> Context:\n",
      ">>>\n",
      "into his office and he said, \"Oh, professor, professor, thank you so much for your \n",
      "machine learning class. I learned so much from it. There's this stuff that I learned in your \n",
      "class, and I now use every day. And it's helped me make lots of money, and here's a \n",
      "picture of my big house.\"  \n",
      "So my friend was very excited. He said, \"Wow. That's great. I'm glad to hear this \n",
      "machine learning stuff was actually useful. So what was it that you learned? Was it \n",
      "logistic regression? Was it the PCA? Was it the data networks? What was it that you \n",
      "learned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \n",
      "So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, \n",
      "and we'll actually have a short MATLAB tutorial in one of\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      ">>>\n",
      "Instructor (Andrew Ng) : Oh, yeah, let's see, right. So our policy has been that you're \n",
      "welcome to use R, but I would strongly advise against it, mainly because in the last \n",
      "problem set, we actually supply some code that will run in Octave but that would be \n",
      "somewhat painful for you to translate into R yourself. So for your other assignments, if \n",
      "you wanna submit a solution in R, that's fine. But I think MATLAB is actually totally \n",
      "worth learning. I know R and MATLAB, and I personally end up using MATLAB quite a \n",
      "bit more often for various reasons.\n",
      ">>>\n",
      "---\n",
      "> Question: what is the policy on R?\n",
      "> Context:\n",
      ">>>\n",
      "So later this quarter, we'll use the discussion sections to talk about things like convex \n",
      "optimization, to talk a little bit about hidden Markov models, which is a type of machine \n",
      "learning algorithm for modeling time series and a few other things, so extensions to the \n",
      "materials that I'll be covering in the main lectures. And attendance at the discussion \n",
      "sections is optional, okay?  \n",
      "So that was all I had from logistics. Before we move on to start talking a bit about \n",
      "machine learning, let me check what questions you have. Yeah?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      ">>>\n",
      "Okay. So one more organizational question. I'm curious, how many of you know \n",
      "MATLAB? Wow, cool, quite a lot. Okay. So as part of the — act ually how many of you \n",
      "know Octave or have used Octave? Oh, okay, much smaller number.  \n",
      "So as part of this class, especially in the homeworks, we'll ask you to implement a few \n",
      "programs, a few machine learning algorithms as part of the homeworks. And most of\n",
      ">>>\n",
      "<|ipynb_marker|> END OF DOC\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining various techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      ">>>\n",
      "those homeworks will be done in either MATLAB or in Octave, which is sort of — I \n",
      "know some people call it a free version of MATLAB, which it sort of is, sort of isn't.  \n",
      "So I guess for those of you that haven't seen MATLAB before, and I know most of you \n",
      "have, MATLAB is I guess part of the programming language that makes it very easy to \n",
      "write codes using matrices, to write code for numerical routines, to move data around, to \n",
      "plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of \n",
      "learning algorithms.  \n",
      "And in case some of you want to work on your own home computer or something if you \n",
      "don't have a MATLAB license, for the purposes of this class, there's also — [inaudible] \n",
      "write that down [inaudible] MATLAB — there' s also a software package called Octave \n",
      "that you can download for free off the Internet. And it has somewhat fewer features than \n",
      "MATLAB, but it's free, and for the purposes of this class, it will work for just about \n",
      "everything.  \n",
      "So actually I, well, so yeah, just a side comment for those of you that haven't seen \n",
      "MATLAB\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "NO_OUTPUT\n",
      "\"\"\"\n",
      "]\n",
      "\n",
      "COMPLETION_PROMPT = [\n",
      "    f\"Question: {question}\\nContext:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:\"\n",
      "]  # noqa: E501\n",
      "EXAMPLES_PROVIDED = 10\n",
      "TEMPERATURE = 0.0\n",
      "MAX_TOKENS = 100\n",
      "STOP = [\"\\n\"]\n",
      "\n",
      "\n",
      "def extract_relevant_parts(question: str, context: str) -> str:\n",
      "    \"\"\"\n",
      "    Given a question and a context, extract the relevant parts of the context that are relevant to answer the question.\n",
      "\n",
      "    Parameters:\n",
      "    question (str): The question to be answered.\n",
      "    context (str): The context from which to extract relevant parts.\n",
      "\n",
      "    Returns:\n",
      "    str: The relevant parts of the context, or \"NO_OUTPUT\" if none of the context is relevant.\n",
      "    \"\"\"\n",
      "    prompt = EXAMPLES + [f\"Question: {question}\\nContext:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:\"]\n",
      "    response = openai.Completion.create(\n",
      "        engine=\"text-davinci-003\",\n",
      "        prompt=prompt,\n",
      "        temperature=TEMPERATURE,\n",
      "        max_tokens=MAX_TOKENS,\n",
      "        stop=STOP,\n",
      "    )\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "NO_OUTPUT\n",
      "----\n",
      "> Question: what did they say about matlab?\n",
      "> Context:\n",
      ">>>\n",
      "So, for example, if you’re using Matlab, you can use the fminunc function, which is a \n",
      "function that will do gradient descent for you. And you can use that to minimize the cost \n",
      "function J of theta. And if you do that, you’ll be able to get back a value of theta that \n",
      "minimizes your cost function. \n",
      ">>>\n",
      "Extracted relevant parts: So, for example, if you’re using Matlab, you can use the fminunc function, which is a function that will do gradient descent for you. And you can use that to minimize the cost function J of theta. And if you do that, you’ll be able to get back a value of theta that minimizes your cost function.\n",
      "----\n",
      "> Question: what did they say about matlab?\n",
      "> Context:\n",
      ">>>\n",
      "So, for example, if you’re using Matlab, you can use the fminunc function, which is a \n",
      "function that will do gradient descent for you. And you can use that to minimize the cost \n",
      "function J of theta. And if you do that, you’ll be able to get back a value of theta that \n",
      "minimizes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "NO_OUTPUT\n",
      "> Question: what did they say about ICA?\n",
      "> Context:\n",
      ">>>\n",
      "machine data, like the magneto-encephalogram would be an EEG data. We'll talk about \n",
      "that more when we go and describe ICA or independent component analysis algorithms, \n",
      "which is what you just saw.\n",
      ">>>\n",
      "Extracted relevant parts: We'll talk about \n",
      "that more when we go and describe ICA or independent component analysis algorithms, \n",
      "which is what you just saw.\"\"\"\n",
      "]\n",
      "\n",
      "EXAMPLES_PROVIDED = len(EXAMPLES)\n",
      "\n",
      "def extract_relevant_parts(question, context):\n",
      "    if EXAMPLES_PROVIDED > 0:\n",
      "        for example in EXAMPLES:\n",
      "            if example.startswith(f\"> Question: {question}\"):\n",
      "                return example.split(\"Extracted relevant parts: \")[1].strip()\n",
      "    return \"NO_OUTPUT\" if question not in context else context.split(question)[1].strip()[:context.split(question)[1].strip().find('.')+1] if '.' in context.split(question)[1].strip() else context.split(question)[1].strip()[:context.split(question)[1].strip().find('\\n')+1] if '\\n' in context.split(question)[1].strip() else context.split(question)[1].strip()[:context.split(question\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other types of retrieval\n",
    "\n",
    "It's worth noting that vectordb as not the only kind of tool to retrieve documents. \n",
    "\n",
    "The `LangChain` retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(\"MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"yourselves. You can also come and talk to me or the TAs if you want to brainstorm ideas \\nwith us.  \\nOkay. So one more organizational question. I'm curious, how many of you know \\nMATLAB? Wow, cool, quite a lot. Okay. So as part of the — act ually how many of you \\nknow Octave or have used Octave? Oh, okay, much smaller number.  \\nSo as part of this class, especially in the homeworks, we'll ask you to implement a few \\nprograms, a few machine learning algorithms as part of the homeworks. And most of  those homeworks will be done in either MATLAB or in Octave, which is sort of — I \\nknow some people call it a free version of MATLAB, which it sort of is, sort of isn't.  \\nSo I guess for those of you that haven't seen MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to \\nwrite codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your own home computer or something if you \\ndon't have a MATLAB license, for the purposes of this class, there's also — [inaudible] \\nwrite that down [inaudible] MATLAB — there' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than \\nMATLAB, but it's free, and for the purposes of this class, it will work for just about \\neverything.\")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
